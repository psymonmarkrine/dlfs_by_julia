# 6章 学習に関するテクニック
## 6.1 パラメータの更新
### 6.1.1 冒険家の話
### 6.1.2 SGD

```julia
mutable struct SGD
    lr::Float
end
SGD() = SGD(0.01)

function update(self::SGD, params, grads)
    for (key,_)=params
        params[key] .-= self.lr * grads[key]
    end
end
```

～  

```julia
network = TwoLayerNet(...)
optimizer = SGD()

for i = 1:10000
    ...
    x_batch, t_batch = get_mini_batch(...) # ミニバッチ
    grads = gradient(network, x_batch, t_batch)
    params = network.params
    update(optimizer, params, grads)
    ...
end
```

～  

### 6.1.3 SGDの欠点
### 6.1.4 Momentum

～  

```julia
mutable struct Momentum # """Momentum SGD"""
    lr::AbstractFloat
    momentum::AbstractFloat
    v
end
 Momentum(lr=0.01, momentum=0.9) = Momentum(lr, momentum, nothing)
         
function update(self::Momentum, params, grads)
    if isnothing(self.v)
        self.v = IdDict()
        for (key, val) = params
            self.v[key] = zero(val)
        end
    end

    for (key,_) = params
        self.v[key] = self.momentum .* self.v[key] .- self.lr * grads[key] 
        params[key] .+= self.v[key]
    end
end
```

～  

### 6.1.5 AdaGrad

～  

```julia
mutable struct AdaGrad # """AdaGrad"""
    lr::AbstractFloat
    h
end
AdaGrad(lr=0.01) = AdaGrad(lr, nothing)
        
function update(self::AdaGrad, params, grads)
    if isnothing(self.h)
        self.h = IdDict()
        for (key, val) = params
            self.h[key] = zero(val)
        end
    end

    for (key,_) = params
        self.h[key] .+= grads[key].^2
        params[key] .-= self.lr * grads[key] ./ (sqrt.(self.h[key]) + 1e-7)
    end
end
```

～  

### 6.1.6 Adam
### 6.1.7 どの更新手法を用いるか？
### 6.1.8 MNISTデータセットによる更新手法の確認
## 6.2 重みの初期値
### 6.2.1 重みの初期値を0にする？
