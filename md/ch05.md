# 5章 誤差逆伝播

前章では、ニューラルネットワークの学習について説明しました。  
～  

## 5.1 計算グラフ
### 5.1.1 計算グラフで解く
### 5.1.2 局所的な計算
### 5.1.3 なぜ計算グラフで解くのか？
## 5.2 連鎖率
### 5.2.1 計算グラフの逆伝播
### 5.2.3 連鎖率と計算グラフ
## 5.3 逆伝播
### 5.3.1 加算ノードの逆伝播
### 5.3.2 乗算ノードの逆伝播
### 5.3.3 リンゴの例
## 5.4 単純なレイヤの実装
### 5.4.1 乗算レイヤの実装

～  

```julia
mutable struct MulLayer
    x
    y
end

function MulLayer()
    return MulLayer(nothing, nothing)
end

function forward(self::MulLayer, x, y)
    self.x = x
    self.y = y                
    out = x * y

    return out
end

function backward(self::MulLayer, dout)
    dx = dout * self.y # xとyをひっくり返す
    dy = dout * self.x

    return dx, dy
end
```

```julia
apple = 100
apple_num = 2
tax = 1.1

# layer
mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = forward(mul_apple_layer, apple, apple_num)
price = forward(mul_tax_layer, apple_price, tax)

print(price) # 220.00000000000003
```

～  

```julia
# backward
dprice = 1
dapple_price, dtax = backward(mul_tax_layer, dprice)
dapple, dapple_num = backward(mul_apple_layer, dapple_price)

print("$dapple, $dapple_num, $dtax") # 2.2, 110.00000000000001, 200
```

### 5.4.2 加算レイヤの実装

～  

```julia
abstract type AddLayer end

function forward(self::AddLayer, x, y)
    out = x + y

    return out
end

function backward(self::AddLayer, dout)
    dx = dout * 1
    dy = dout * 1

    return dx, dy
end
```

～  

```julia
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1

# layer
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = forward(mul_apple_layer, apple, apple_num)  # (1)
orange_price = forward(mul_orange_layer, orange, orange_num)  # (2)
all_price = forward(add_apple_orange_layer, apple_price, orange_price)  # (3)
price = forward(mul_tax_layer, all_price, tax)  # (4)

# backward
dprice = 1
dall_price, dtax = backward(mul_tax_layer, dprice)  # (4)
dapple_price, dorange_price = backward(add_apple_orange_layer, dall_price)  # (3)
dorange, dorange_num = backward(mul_orange_layer, dorange_price)  # (2)
dapple, dapple_num = backward(mul_apple_layer, dapple_price)  # (1)

print(price) # 715.0000000000001
print(dapple_num, dapple, dorange, dorange_num, dtax) # 110.00000000000001 2.2 3.3000000000000003 165.0 650
```

## 5.5 活性化関数レイヤの実装
### 5.5.1 ReLUレイヤ

～  

```julia
mutable struct Relu
    mask
end

Relu() = Relu(nothing)

function forward(self::Relu, x)
        self.mask = (x .<= 0)
        out = copy(x)
        out[self.mask] = 0

        return out
end

function backward(self::Relu, dout)
        dout[self.mask] = 0
        dx = dout

        return dx
end
```

～  

```julia
julia> x = [1.0 -2.0; -0.5 3.0]
2×2 Matrix{Float64}:
  1.0  -2.0
 -0.5   3.0

julia> x = [1.0 -0.5; -2.0 3.0]
2×2 Matrix{Float64}:
  1.0  -0.5
 -2.0   3.0

julia> mask = (x.<=0)
2×2 BitMatrix:
 0  1
 1  0
```

### 5.5.2 Sigmoidレイヤ

```julia
mutable struct Sigmoid
    out
end

Sigmoid() = Sigmoid(nothing)

function forward(self::Sigmoid, x)
    out = 1 ./ (1 .+ exp.(x))
    self.out = out

    return out
end

function backward(self, dout)
    dx = dout .* (1.0 .- self.out) .* self.out

    return dx
end
```

## 5.6 Affine/Softmaxレイヤの実装
### 5.6.1 Affineレイヤ

～  

```julia
julia> X = rand(2);    # 入力

julia> W = rand(3, 2); # 重み

julia> B = rand(3);    # バイアス

julia>

julia> size(X)
(2,)

julia> size(W)
(3, 2)

julia> size(B)
(3,)

julia>

julia> Y = W*X + B;
```

～  

### 5.6.2 バッチ版Affineレイヤ

～  

```julia
julia> X_dot_W = [0 0 0; 10 10 10]
2×3 Matrix{Int64}:
  0   0   0
 10  10  10

julia> B = [1 2 3]
1×3 Matrix{Int64}:
 1  2  3
```

～  

```julia
julia> dY = [1 2 3; 4 5 6]
2×3 Matrix{Int64}:
 1  2  3
 4  5  6

julia> dB = sum(dY, dims=1)
1×3 Matrix{Int64}:
 5  7  9
```

～  

```julia
mutable struct Affine
    W
    b
    x
    dW
    db
end

function Affine(W, b)
    Affine(W, b, nothing, nothing, nothing)
end

function forward(self::Affine, x)
    self.x = x
    out = self.x * self.W .+ self.b

    return out
end

function backward(self::Affine, dout)
    dx = dout * self.W'
    self.dW = self.x' * dout
    self.db = np.sum(dout, dims=1)
    
    return dx
end
```

### 5.6.3 Softmax-with-Lossレイヤ

～  

```julia
mutable struct SoftmaxWithLoss
    loss
    y # softmaxの出力
    t # 教師データ
end

SoftmaxWithLoss() = SoftmaxWithLoss(nothing, nothing, nothing)

function forward(self::SoftmaxWithLoss, x, t)
    self.t = t
    self.y = softmax(x)
    self.loss = cross_entropy_error(self.y, self.t)
    
    return self.loss
end

function backward(self::SoftmaxWithLoss, dout=1)
    batch_size = size(self.t, 1)
    dx = (self.y - self.t) ./ batch_size

    return dx
end
```

## 5.7 誤差逆伝播法の実装
### 5.7.1 ニューラルネットワークの学習の全体図
### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装



### 5.7.3 誤差逆伝播法の勾配確認



### 5.7.4 誤差逆伝播法を使った学習



## 5.8 まとめ

～  
