# 4章 ニューラルネットワークの学習

本章のテーマは、ニューラルネットワークの学習です。  
～  

## 4.1 データから学習する
### 4.1.1 データ駆動
### 4.1.2 訓練データとテストデータ
## 4.2 損失関数
### 4.2.1 2乗和誤差

損失関数として用いられる関数はいくつかありますが、もっとも有名ななものは**2乗和誤差**（sum of squared error）でしょう。  
～  

```julia
julia> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0];

julia> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0];

```

～  

```julia
function sum_squared_error(y, t)
    return 0.5 * sum((y-t).^2)
end
```

～  

```julia
julia> # 「3」を正解とする

julia> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0];

julia>

julia> # 例１：「3」の確率が最も高い場合（0.6）

julia> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0];

julia> sum_squared_error(y, t)
0.09750000000000003

julia>

julia> # 例１：「7」の確率が最も高い場合（0.6）

julia> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0];

julia> sum_squared_error(y, t)
0.5974999999999999
```

ここでは2つの例を示しています。  
～  

### 4.2.2 交差エントロピー誤差

2乗和誤差と別の損失関数として、**交差エントロピー誤差**（cross entropy error）もよく用いられます。  
～  

```julia
julia> function cross_entropy_error(y, t)
           delta = 1.e-7
           return -sum(t .* log.(y .+ delta))
       end
cross_entropy_error (generic function with 1 method)

```

～  

```julia
julia> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0];

julia> y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0];

julia> cross_entropy_error(y, t)
0.510825457099338

julia>

julia> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0];

julia> cross_entropy_error(y, t)
2.302584092994546
```

～  

### 4.2.3 ミニバッチ処理

～  

```julia
include("dataset/mnist.jl") # load_mnist()

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=true, one_hot_label=true)

print(size(x_train)) # (60000, 784)
print(size(t_train)) # (60000, 10)
```

～  
それでは、この訓練データの中からランダムに10枚だけ抜き出すには、どうすればよいのでしょうか？  
～  

```julia
import Random: shuffle
train_size = size(t_train, 1)
batsh_size = 10
batch_mask = shuffle(1:train_size)[1:batch_size]
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

～  

```julia
julia> import Random: shuffle

julia> shuffle(1:60000)[1:10]
10-element Vector{Int64}:
  8130
 17124
 36096
 52441
 21595
 23334
 25369
 46353
 52087
  6978
```

### 4.2.4 \[バッチ対応版\] 交差エントロピー誤差の実装

では、ミニバッチのようなデータに対応した交差エントロピー誤差はどのように実装できるでしょうか？  
これは、先ほど実装した交差エントロピー誤差──それはひとつのデータを対象とした誤差でした──を改良することで簡単に実装できます。  
ここでは、データがひとつの場合と、データがバッチとしてまとめられて入力される場合の両方のケースを多重ディスパッチで実装します。  

```julia
julia> function cross_entropy_error(y::Vector{T}, t) where T
            delta = 1.e-7
            return -sum(t .* log.(y .+ delta))
       end
cross_entropy_error (generic function with 1 method)

julia> function cross_entropy_error(y::Matrix{T}, t) where T
            batch_size = size(y, 1)
            return sum(cross_entropy_error.([y[i,:] for i=1:batch_size], [t[i,:] for i=1:batch_size])) / batch_size
       end
cross_entropy_error (generic function with 2 methods)
```

～  
また、教師データがラベルとして与えられたとき（one-hot表現ではなく、「2」や「7」といったラベルとして与えられたとき）、交差エントロピー誤差は次のように実装することができます。  

```julia
function cross_entropy_error(y::Vector{T}, t::Y) where T <: Real where Y <: Integer
    delta = 1.e-7
    return -log.(y[t] .+ delta)
end

function cross_entropy_error(y::Matrix{T}, t::Vector{Y}) where T <: Real where Y <: Integer
    batch_size = size(y, 1)
    return sum(cross_entropy_error.([y[i,:] for i=1:batch_size], [t[i] for i=1:batch_size])) / batch_size
end
```

実装のポイントは、one-hot表現で`t`が0の要素は、交差エントロピー誤差も0であるから、その計算は無視してもよいということです。  
～  

### 4.2.5 なぜ損失関数を設定するのか？

～  

## 4.3 数値微分
### 4.3.1 微分

～  

```julia
# 悪い実装例
function numerical_diff(f, x)
    h = 1e-50
    return (f(x+h) - f(x)) / h
end
```

～  

```julia
julia> Float32(10e-50)
0.0f0
```

～  

```julia
function numerical_diff(f, x)
    h = 1e-4
    return (f(x+h) - f(x-h)) / 2h
end
```

### 4.3.2 数値微分の例

～  

```julia
function function_1(x)
    return 0.01x^2 + 0.1x
end
```

続いてこの関数を描画します。  
～  

```julia
using Plots

x = 0:0.1:20
y = function_1.(x)

plot(xlabel="x", ylabel="f(x)", leg=false)
plot!(x, y)
```

～  

```julia
julia> numerical_diff(function_1, 5)
0.1999999999990898

julia> numerical_diff(function_1, 10)
0.2999999999986347
```

### 4.3.3 偏微分

～  

```julia
function function_2(x)
    return x[0]^2 + x[1]^2
    # または return sum(x.^2)
end
```

～  

**問1**：x0=3、x1=4のときのx0に対する偏微分を求めよ。  

```julia
julia> function function_tmp1(x0)
           return x0^2 + 4.0^2
       end
function_tmp1 (generic function with 1 method)

julia> numerical_diff(function_tmp1, 3.0)
6.00000000000378
```

**問2**：x0=3、x1=4のときのx1に対する偏微分を求めよ。  

```julia
julia> function function_tmp2(x1)
           return 3.0^2 + x1^2
       end
function_tmp2 (generic function with 1 method)

julia> numerical_diff(function_tmp2, 4.0)
7.999999999999119
```

これらの問題では、変数がひとつだけの関数を定義して、その関数について微分を求めるような実装を行っています。  
～  
