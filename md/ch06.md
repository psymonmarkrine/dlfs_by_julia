# 6章 学習に関するテクニック
## 6.1 パラメータの更新
### 6.1.1 冒険家の話
### 6.1.2 SGD

```julia
mutable struct SGD
    lr::Float
end
SGD() = SGD(0.01)

function update(self::SGD, params, grads)
    for (key,_)=params
        params[key] .-= self.lr * grads[key]
    end
end
```

～  

```julia
network = TwoLayerNet(...)
optimizer = SGD()

for i = 1:10000
    ...
    x_batch, t_batch = get_mini_batch(...) # ミニバッチ
    grads = gradient(network, x_batch, t_batch)
    params = network.params
    update(optimizer, params, grads)
    ...
end
```

～  

### 6.1.3 SGDの欠点
### 6.1.4 Momentum

～  

```julia
mutable struct Momentum # """Momentum SGD"""
    lr::AbstractFloat
    momentum::AbstractFloat
    v
end
 Momentum(lr=0.01, momentum=0.9) = Momentum(lr, momentum, nothing)
         
function update(self::Momentum, params, grads)
    if isnothing(self.v)
        self.v = IdDict()
        for (key, val) = params
            self.v[key] = zero(val)
        end
    end

    for (key,_) = params
        self.v[key] = self.momentum .* self.v[key] .- self.lr * grads[key] 
        params[key] .+= self.v[key]
    end
end
```

～  

### 6.1.5 AdaGrad

～  

```julia
mutable struct AdaGrad # """AdaGrad"""
    lr::AbstractFloat
    h
end
AdaGrad(lr=0.01) = AdaGrad(lr, nothing)
        
function update(self::AdaGrad, params, grads)
    if isnothing(self.h)
        self.h = IdDict()
        for (key, val) = params
            self.h[key] = zero(val)
        end
    end

    for (key,_) = params
        self.h[key] .+= grads[key].^2
        params[key] .-= self.lr * grads[key] ./ (sqrt.(self.h[key]) + 1e-7)
    end
end
```

～  

### 6.1.6 Adam
### 6.1.7 どの更新手法を用いるか？
### 6.1.8 MNISTデータセットによる更新手法の確認
## 6.2 重みの初期値
### 6.2.1 重みの初期値を0にする？
### 6.2.2 隠れ層のアクティベーション分布

```julia
import OrderedCollections: OrderedDict
using Plots

function sigmoid(x)
    return 1 / (1 + exp(-x))
end

node_num = 100              # 各隠れ層のノード（ニューロン）の数
hidden_layer_size = 5       # 隠れ層が5層
activations = OrderedDict() # ここにアクティベーションの結果を格納する

for i = 1:hidden_layer_size
    if i != 1
        x = activations[i-1]
    else
        x = randn(1000, 100)
    end

    w = randn(node_num, node_num) * 1


    z = x * w
    a = sigmoid.(a) # シグモイド関数！
    activations[i] = a
end
```

～  

```julia
# ヒストグラムを描画
p = []
for (i, a) = activations    
    push!(p, histogram!(a[:], bins=30, xlim=(0,1), title="$i-layer", leg=false))
end
plot(p..., layout=(1,length(activations)))
```

![fig6-10](../image/ch06/fig06-10.png)  
図6-10　重みの初期値として標準偏差1のガウス分布を用いたときの、各層のアクティベーションの分布  

～  

```julia
# w = randn(node_num, node_num) * 1
w = randn(node_num, node_num) * 0.01
```

![fig6-11](../image/ch06/fig06-11.png)  
図6-11　重みの初期値として標準偏差0.01のガウス分布を用いたときの、各層のアクティベーションの分布  

～  

```julia
node_num = 100  # 前層のノードの数
w = randn(node_num, node_num) / sqrt(node_num)
```

![fig6-13](../image/ch06/fig06-13.png)  
図6-13　重みの初期値として「Xavierの初期値」を用いたときの、各層のアクティベーションの分布  

～  

### 6.2.3 ReLUの場合の重みの初期値

～  

![fig6-14a](../image/ch06/fig06-14a.png)  
![fig6-14b](../image/ch06/fig06-14b.png)  
![fig6-14c](../image/ch06/fig06-14c.png)  
